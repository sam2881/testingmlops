from pyspark.sql.functions import col, explode_outer

def flatten(df):
    """
    Flattens a DataFrame by expanding nested structures and arrays into top-level columns,
    maintaining the full hierarchical dot-separated paths as column aliases.
    """
    complex_fields = dict([(field.name, field.dataType)
                           for field in df.schema.fields
                           if isinstance(field.dataType, (ArrayType, StructType))])
    
    while complex_fields:
        next_complex_fields = {}
        for col_name, data_type in complex_fields.items():
            # If StructType, convert all sub-elements to columns with hierarchical names
            if isinstance(data_type, StructType):
                for field in data_type.fields:
                    new_col_name = f"{col_name}.{field.name}"
                    df = df.withColumn(new_col_name, col(f"{col_name}.{field.name}"))
                df = df.drop(col_name)

            # If ArrayType with StructType elements, explode the array and flatten
            elif isinstance(data_type, ArrayType) and isinstance(data_type.elementType, StructType):
                df = df.withColumn(col_name, explode_outer(col(col_name)))
                for field in data_type.elementType.fields:
                    new_col_name = f"{col_name}.{field.name}"
                    df = df.withColumn(new_col_name, col(f"{col_name}.{field.name}"))
                df = df.drop(col_name)

            # Track any newly expanded fields for further processing in the next iteration
            for field in df.schema.fields:
                if isinstance(field.dataType, (ArrayType, StructType)) and field.name not in complex_fields:
                    next_complex_fields[field.name] = field.dataType

        complex_fields = next_complex_fields

    return df
