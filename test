def determine_explode_paths(column_paths):
    needs_explode = []
    for path in column_paths:
        if 'Array' in path:  # Replace this with your actual condition to identify array columns.
            needs_explode.append(path)
    return needs_explode

def apply_transformations(df, column_paths):
    needs_explode = determine_explode_paths(column_paths)
    for path in needs_explode:
        exploded_column = path.split('.')[-1]  # Simplified assumption; adjust as necessary.
        df = df.withColumn(exploded_column, explode_outer(path))
    return df

def generate_select_expression_from_list(column_names):
    seen_aliases = {}
    select_expr = []
    for col_name in column_names:
        parts = col_name.split('.')
        simple_name = parts[-1]
        snake_case_name = camel_to_snake(simple_name)
        if snake_case_name in seen_aliases:
            count = seen_aliases[snake_case_name] + 1
            snake_case_name += f"_{count}"
            seen_aliases[snake_case_name] = count
        else:
            seen_aliases[snake_case_name] = 1
        select_expr.append(f"col('{col_name}').alias('{snake_case_name}')")
    return select_expr

def generate_pyspark_script(config, schema):
    schema_file_path = config['pyspark_schema_path']
    output_file_path = config['pyspark_script_output_path']
    schema_module = Path(schema_file_path).stem

    # Generate schema file
    generate_schema_file(schema, schema_file_path)
    column_paths = extract_column_paths(schema)

    # Generate the actual PySpark script
    pyspark_code = f"""
from pyspark.sql import SparkSession
from {schema_module} import get_schema
from pyspark.sql.functions import col, explode_outer

spark = SparkSession.builder.appName("JsonToDataFrame").getOrCreate()

schema = get_schema()
df = spark.read.schema(schema).json("{config['json_data_input_path']}")

df_transformed = apply_transformations(df, column_paths)

select_expressions = generate_select_expression_from_list(column_paths)
select_statement = df_transformed.select(*select_expressions)

select_statement.show(truncate=False)
"""
    with open(output_file_path, 'w') as file:
        file.write(pyspark_code)

def main():
    config_path = 'config.yaml'
    with open(config_path, 'r') as file:
        config = yaml.safe_load(file)

    json_schema = read_json_file(config['json_schema_path'])
    definitions = json_schema.get('definitions', {})
    pyspark_schema = convert_to_spark_schema(json_schema, definitions)
    generate_pyspark_script(config, pyspark_schema)

if __name__ == "__main__":
    main()
