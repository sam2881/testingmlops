from pyspark.sql.functions import col, explode_outer

def flatten(df):
    """
    Flattens a DataFrame by expanding nested structures and arrays into top-level columns,
    while preserving the original dot-separated column names. This version retains the full path without introducing
    underscores, which may help in maintaining clarity in highly nested data schemas.
    """
    complex_fields = [(field.name, field.dataType) for field in df.schema.fields
                      if isinstance(field.dataType, (ArrayType, StructType))]

    while complex_fields:
        col_name, data_type = complex_fields.pop(0)
        if isinstance(data_type, StructType):
            # Expand struct fields directly
            for field in data_type.fields:
                full_field_name = f"{col_name}.{field.name}"
                df = df.withColumn(full_field_name, col(full_field_name))
                complex_fields.append((full_field_name, field.dataType))

        elif isinstance(data_type, ArrayType) and isinstance(data_type.elementType, StructType):
            # Handle arrays by exploding them first
            df = df.withColumn(col_name, explode_outer(col(col_name)))
            # Then expand their struct fields
            for field in data_type.elementType.fields:
                full_field_name = f"{col_name}.{field.name}"
                df = df.withColumn(full_field_name, col(full_field_name))
                complex_fields.append((full_field_name, field.dataType))

    return df
