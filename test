from pyspark.sql.functions import col, explode_outer

def flatten(df, root_name=''):
    """
    Recursively flattens the structure and arrays in the DataFrame, maintaining complete hierarchical paths.
    """
    # Finding complex fields in the DataFrame schema
    complex_fields = {f.name: f.dataType for f in df.schema.fields if isinstance(f.dataType, (ArrayType, StructType))}
    
    # Processing each complex field
    while complex_fields:
        new_complex_fields = {}
        for field_name, dtype in complex_fields.items():
            full_field_name = f"{root_name}.{field_name}".strip('.')
            
            if isinstance(dtype, StructType):
                # Flatten StructType fields into columns
                for nested_field in dtype.fields:
                    nested_col_name = f"{full_field_name}.{nested_field.name}"
                    df = df.withColumn(nested_col_name, col(f"{full_field_name}.{nested_field.name}"))
                df = df.drop(full_field_name)
            
            elif isinstance(dtype, ArrayType) and isinstance(dtype.elementType, StructType):
                # Explode Arrays and recursively flatten
                df = df.withColumn(full_field_name, explode_outer(col(full_field_name)))
                for nested_field in dtype.elementType.fields:
                    nested_col_name = f"{full_field_name}.{nested_field.name}"
                    df = df.withColumn(nested_col_name, col(f"{full_field_name}.{nested_field.name}"))
                df = df.drop(full_field_name)
        
            # Track newly expanded fields for further processing
            for new_field in df.schema.fields:
                if isinstance(new_field.dataType, (ArrayType, StructType)) and new_field.name not in complex_fields:
                    new_complex_fields[new_field.name] = new_field.dataType
        
        complex_fields = new_complex_fields

    return df
