from pyspark.sql.functions import col, explode_outer

def flatten(df):
    """
    Flattens a DataFrame by expanding nested structures and arrays into top-level columns,
    while maintaining the original dot-separated column names. Avoids immediate drops to potentially improve performance.
    """
    complex_fields = [(field.name, field.dataType) for field in df.schema.fields
                      if isinstance(field.dataType, (ArrayType, StructType))]

    while complex_fields:
        col_name, data_type = complex_fields.pop(0)
        if isinstance(data_type, StructType):
            # Expand struct fields directly
            for field in data_type.fields:
                df = df.withColumn(f"{col_name}.{field.name}", col(f"{col_name}.{field.name}"))
        
        elif isinstance(data_type, ArrayType) and isinstance(data_data.elementType, StructType):
            # Handle arrays by exploding them and expanding their fields
            df = df.withColumn(col_name, explode_outer(col(col_name)))
            for field in data_data.elementType.fields:
                df = df.withColumn(f"{col_name}.{field.name}", col(f"{col_name}.{field.name}"))

    # Consider dropping columns after all transformations if necessary
    return df
