from pyspark.sql.functions import col, explode_outer

def flatten_df(df):
    """
    Iteratively flatten a DataFrame with nested structures or arrays,
    maintaining column names and paths.
    """
    # Queue to manage columns to process
    queue = []
    # Initialize queue with root level columns
    for field in df.schema.fields:
        queue.append((field.dataType, field.name))
    
    # Process queue
    while queue:
        dtype, path = queue.pop(0)
        
        if isinstance(dtype, StructType):
            # Enqueue nested StructType fields
            for field in dtype.fields:
                new_path = f"{path}.{field.name}"
                queue.append((field.dataType, new_path))
        elif isinstance(dtype, ArrayType) and isinstance(dtype.elementType, StructType):
            # Explode arrays of StructType and enqueue fields
            df = df.withColumn(path, explode_outer(col(path)))
            for field in dtype.elementType.fields:
                new_path = f"{path}.{field.name}"
                queue.append((field.dataType, new_path))
        else:
            # Simple types are directly included in the DataFrame's select expression
            df = df.withColumn(path, col(path))
    
    # Generate select expression to rename paths to column-friendly names
    select_expr = [col(path).alias(path.replace('.', '_')) for _, path in queue]
    return df.select(*select_expr)

# Usage
df_flattened = flatten_df(df)
df_flattened.printSchema()
