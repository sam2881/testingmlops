from pyspark.sql.functions import col, explode_outer

def flatten_df(df, alias_dict={}, parent_prefix=""):
    """
    Flatten the DataFrame schema by expanding nested structures and arrays.
    Maintains a dictionary of aliases to ensure columns are uniquely and consistently named.
    """
    for field in df.schema.fields:
        name = field.name
        dtype = field.dataType
        if isinstance(dtype, StructType):
            # For Struct fields, recurse and flatten further
            df = flatten_df(df, alias_dict, f"{parent_prefix}{name}.")
        elif isinstance(dtype, ArrayType) and isinstance(dtype.elementType, StructType):
            # For ArrayType(StructType) fields, explode and then flatten
            path = f"{parent_prefix}{name}"
            df = df.withColumn(name, explode_outer(col(path)))
            df = flatten_df(df, alias_dict, path + ".")
        else:
            # Add field to alias dictionary
            final_path = f"{parent_prefix}{name}"
            if final_path not in alias_dict:
                alias_dict[final_path] = name  # Keep track of original paths to names

    # Select columns using aliases to maintain flat structure
    select_expr = [col(key).alias(alias_dict[key].replace('.', '_')) for key in alias_dict]
    return df.select(*select_expr)

# Usage
df_flattened = flatten_df(df)
df_flattened.printSchema()
