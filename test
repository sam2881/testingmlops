def generate_pyspark_script(config, schema):
    schema_json = json.dumps(schema.jsonValue(), indent=4)
    schema_json = schema_json.replace('true', 'True').replace('false', 'False')
    schema_file_path = config['pyspark_schema_path']
    output_file_path = config['pyspark_script_output_path']
    schema_file_stem = Path(schema_file_path).stem

    with open(schema_file_path, 'w') as schema_file:
        schema_file.write(f"""
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, ArrayType, MapType, BooleanType, LongType, DoubleType
from pyspark.sql.functions import col, explode_outer

def get_schema():
    return StructType.fromJson({schema_json})
""")

    pyspark_code = f"""
from pyspark.sql import SparkSession
from {schema_file_stem} import get_schema

spark = SparkSession.builder.appName("JsonToDataFrame").getOrCreate()
df = spark.read.schema(get_schema()).json("{config['json_data_input_path']}")

df_flattened = flatten(df)

# Select and alias columns as required
select_statement = df_flattened.select(
    {generate_select_expression(df_flattened)}
)

# Show the results
select_statement.show(truncate=False)
"""

    with open(output_file_path, 'w') as file:
        file.write(pyspark_code)

def main():
    config_path = 'config.yaml'
    with open(config_path, 'r') as file:
        config = yaml.safe_load(file)

    json_schema = read_json_file(config['json_schema_path'])
    definitions = json_schema.get('definitions', {})
    pyspark_schema = convert_to_spark_schema(json_schema, definitions)
    generate_pyspark_script(config, pyspark_schema)

if __name__ == "__main__":
    main()
