from pyspark.sql.functions import col, explode_outer

def flatten(df):
    """
    Flatten a DataFrame by expanding nested structures and arrays into top-level columns,
    while maintaining the original dot-separated column names.
    """
    # Track fields to be processed with their original paths
    complex_fields = dict([(field.name, field.dataType)
                           for field in df.schema.fields
                           if isinstance(field.dataType, (ArrayType, StructType))])
    
    while complex_fields:
        # Process one complex field at a time
        for col_name, data_type in complex_fields.items():
            # Flatten StructType fields
            if isinstance(data_type, StructType):
                expanded = [col(f"{col_name}.{field.name}").alias(f"{col_name}.{field.name}")
                            for field in data_type.fields]
                df = df.select("*", *expanded).drop(col_name)

            # Explode and flatten ArrayType fields that contain StructType
            elif isinstance(data_type, ArrayType) and isinstance(data_data.elementType, StructType):
                df = df.withColumn(col_name, explode_outer(col(col_name)))
                expanded = [col(f"{col_name}.{field.name}").alias(f"{col_name}.{field.name}")
                            for field in data_type.elementType.fields]
                df = df.select("*", *expanded)

        # Refresh complex_fields to catch any new structs or arrays made accessible by previous operations
        complex_fields = dict([(field.name, field.dataType)
                               for field in df.schema.fields
                               if isinstance(field.dataType, (ArrayType, StructType))
                               and f"{field.name}" not in complex_fields])

    return df
