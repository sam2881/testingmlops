import json
import re
import yaml
from pathlib import Path
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, explode_outer
from pyspark.sql.types import *

def read_json_file(file_path):
    with open(file_path, 'r') as file:
        return json.load(file)

def resolve_reference(ref, definitions):
    if isinstance(ref, dict) and '$ref' in ref:
        ref_key = ref['$ref'].replace('#/definitions/', '')
        return resolve_reference(definitions.get(ref_key, {}), definitions)
    return ref

def convert_to_spark_schema(json_schema, definitions):
    if isinstance(json_schema, dict):
        if 'type' not in json_schema:
            if '$ref' in json_schema:
                return convert_to_spark_schema(resolve_reference(json_schema['$ref'], definitions), definitions)
            return None

        data_type = json_schema['type']
        if data_type == 'object':
            fields = [StructField(field, convert_to_spark_schema(prop, definitions), True)
                      for field, prop in json_schema.get('properties', {}).items() if convert_to_spark_schema(prop, definitions)]
            return StructType(fields)
        elif data_type == 'array':
            element_schema = json_schema.get('items', {})
            spark_element_schema = convert_to_spark_schema(resolve_reference(element_schema, definitions), definitions)
            return ArrayType(spark_element_schema, True) if spark_element_schema else None
        else:
            return StringType()  # Default for unspecified complex types
    elif isinstance(json_schema, list):
        return convert_to_spark_schema(json_schema[0], definitions) if json_schema else None
    return None

def flatten(df):
    complex_fields = dict([(field.name, field.dataType)
                           for field in df.schema.fields
                           if isinstance(field.dataType, (ArrayType, StructType))])
    while complex_fields:
        col_name = list(complex_fields.keys())[0]
        if isinstance(complex_fields[col_name], StructType):
            expanded = [col(f"{col_name}.{field.name}").alias(f"{col_name}.{field.name}") for field in complex_fields[col_name]]
            df = df.select("*", *expanded).drop(col_name)
        elif isinstance(complex_fields[col_name], ArrayType):
            df = df.withColumn(col_name, explode_outer(col_name))
        complex_fields = dict([(field.name, field.dataType)
                               for field in df.schema.fields
                               if isinstance(field.dataType, (ArrayType, StructType))])
    return df

def generate_pyspark_script(config, schema):
    schema_json = json.dumps(schema.jsonValue(), indent=4)
    schema_json = schema_json.replace('true', 'True').replace('false', 'False')
    schema_file_path = config['pyspark_schema_path']
    output_file_path = config['pyspark_script_output_path']
    schema_file_stem = Path(schema_file_path).stem

    with open(schema_file_path, 'w') as schema_file:
        schema_file.write(f"""
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, ArrayType, MapType, BooleanType, LongType, DoubleType

def get_schema():
    return StructType.fromJson({schema_json})
""")

    pyspark_code = f"""
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, explode_outer
from {schema_file_stem} import get_schema

spark = SparkSession.builder.appName("JsonToDataFrame").getOrCreate()
df = spark.read.schema(get_schema()).json("{config['json_data_input_path']}")

df_flattened = flatten(df)
final_df = df_flattened  # Assuming you have a transformation or selection step here

final_df.show(truncate=False)
"""

    with open(output_file_path, 'w') as file:
        file.write(pyspark_code)

def main():
    config_path = 'config.yaml'
    with open(config_path, 'r') as file:
        config = yaml.safe_load(file)

    json_schema = read_json_file(config['json_schema_path'])
    definitions = json_schema.get('definitions', {})
    pyspark_schema = convert_to_spark_schema(json_schema, definitions)
    generate_pyspark_script(config, pyspark_schema)

if __name__ == "__main__":
    main()
