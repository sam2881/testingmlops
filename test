def generate_pyspark_script(config, schema):
    schema_file_path = config['pyspark_schema_path']
    output_file_path = config['pyspark_script_output_path']
    schema_module = Path(schema_file_path).stem

    # Generate schema file
    generate_schema_file(schema, schema_file_path)

    # Extract column paths and determine paths that need explosion
    list_col = extract_column_paths(schema)
    explode_paths = determine_explode_paths(list_col)  # Ensure this function is defined to find paths needing explosion

    # Create the PySpark script
    pyspark_code = f"""
from pyspark.sql import SparkSession
from {schema_module} import get_schema
from pyspark.sql.functions import col, explode_outer

spark = SparkSession.builder.appName("JsonToDataFrame").getOrCreate()
schema = get_schema()
df = spark.read.schema(schema).option("multiline", "true").json("{config['json_data_input_path']}")

# Exploding necessary arrays
"""
    # Adding explosion code dynamically
    for path in explode_paths:
        pyspark_code += f'df = df.withColumn("{path.split(".")[-1]}_exploded", explode_outer("{path}"))\n'

    # Adding selection expressions
    pyspark_code += f"""
# Generate select expressions
select_expressions = generate_select_expression_from_list([f"{path}_exploded" for path in explode_paths])

# Select and alias columns as required
select_statement = df.select(*select_expressions)

# Show the results
select_statement.show(truncate=False)
"""
