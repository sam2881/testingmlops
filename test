import json
import re
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, explode_outer
from pyspark.sql.types import *

# Utility Functions
def read_json_file(file_path):
    """Read and return the JSON schema from a file."""
    with open(file_path, 'r') as file:
        return json.load(file)

def resolve_reference(ref, definitions):
    """Resolve $ref to actual defined schema, handling nested references."""
    if isinstance(ref, dict) and '$ref' in ref:
        ref_key = ref['$ref'].replace('#/definitions/', '')
        return resolve_reference(definitions.get(ref_key, {}), definitions)
    return ref

def convert_to_spark_schema(json_schema, definitions):
    """Convert JSON schema to PySpark schema, handling types and references."""
    if isinstance(json_schema, dict):
        if 'type' not in json_schema:
            if '$ref' in json_schema:
                return convert_to_spark_schema(resolve_reference(json_schema['$ref'], definitions), definitions)
            return None  # Handle unresolved schemas
        data_type = json_schema['type']
        if data_type == 'object':
            fields = [StructField(field, convert_to_spark_schema(prop, definitions), True)
                      for field, prop in json_schema.get('properties', {}).items() if convert_to_spark_schema(prop, definitions)]
            return StructType(fields)
        elif data_type == 'array':
            element_schema = json_schema.get('items', {})
            spark_element_schema = convert_to_spark_schema(resolve_reference(element_schema, definitions), definitions)
            return ArrayType(spark_element_schema, True) if spark_element_schema else None
        elif data_type == 'string':
            return StringType()
        elif data_type == 'integer':
            return IntegerType()
        elif data_type == 'boolean':
            return BooleanType()
        elif data_type == 'number':
            return DoubleType()
        else:
            return StringType()
    elif isinstance(json_schema, list):
        return convert_to_spark_schema(json_schema[0], definitions) if json_schema else None
    return None

def camel_to_snake(name):
    """Convert camelCase string to snake_case string."""
    s1 = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', name)
    return re.sub('([a-z0-9])([A-Z])', r'\1_\2', s1).lower()

def extract_column_paths(schema, parent_path=""):
    """Recursively extract column paths from a DataFrame schema."""
    column_paths = []
    for field in schema.fields:
        current_path = f"{parent_path}.{field.name}" if parent_path else field.name
        if isinstance(field.dataType, StructType):
            column_paths += extract_column_paths(field.dataType, current_path)
        elif isinstance(field.dataType, ArrayType) and isinstance(field.dataType.elementType, StructType):
            column_paths += extract_column_paths(field.dataType.elementType, current_path)
        else:
            column_paths.append(current_path)
    return column_paths

def flatten(df):
    """Flatten a DataFrame with nested structures or arrays."""
    complex_fields = dict([(field.name, field.dataType)
                           for field in df.schema.fields
                           if type(field.dataType) == ArrayType or type(field.dataType) == StructType])
    while complex_fields:
        col_name = list(complex_fields.keys())[0]
        if isinstance(complex_fields[col_name], StructType):
            expanded = [col(f"{col_name}.{k}").alias(f"{col_name}_{k}") for k in [n.name for n in complex_fields[col_name]]]
            df = df.select("*", *expanded).drop(col_name)
        elif isinstance(complex_fields[col_name], ArrayType):
            df = df.withColumn(col_name, explode_outer(col_name))
        complex_fields = dict([(field.name, field.dataType)
                               for field in df.schema.fields
                               if type(field.dataType) == ArrayType or type(field.dataType) == StructType])
    return df

def generate_select_expression(df):
    """Generate a DataFrame select expression with properly aliased columns, handling duplicates."""
    seen_aliases = set()
    select_expr = []
    for col_name in df.columns:
        parts = col_name.split('.')
        simple_name = camel_to_snake(parts[-1])
        if simple_name in seen_aliases:
            parent_name = camel_to_snake(parts[-2]) if len(parts) > 1 else 'base'
            alias_name = f"{parent_name}_{simple_name}"
        else:
            alias_name = simple_name
        seen_aliases.add(alias_name)
        select_expr.append(col(".".join(parts)).alias(alias_name))
    return df.select(*select_expr)

# Main Execution Flow
spark = SparkSession.builder.appName("Complex JSON Schema Processing").getOrCreate()
file_path = 'json_schema.json'
schema_dict = read_json_file(file_path)
pyspark_schema = generate_pyspark_schema(schema_dict)
df = spark.read.schema(pyspark_schema).json("json_data.json")
df_flattened = flatten(df)
df_selected = generate_select_expression(df_flattened)
df_selected.show()
