import json
import re
import yaml
from pathlib import Path
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType, BooleanType, DoubleType,
    ArrayType, NullType
)
from pyspark.sql.functions import col

def infer_data_type(json_data):
    """
    Infer the data type of JSON data. Supports basic JSON types and maps them to string labels.
    """
    if isinstance(json_data, dict):
        return "object"
    elif isinstance(json_data, list):
        return "array"
    elif isinstance(json_data, str):
        return "string"
    elif isinstance(json_data, int):
        return "integer"
    elif isinstance(json_data, float):
        return "number"
    elif isinstance(json_data, bool):
        return "boolean"
    else:
        return "null"

def _generate_json_schema_recursive(data, refs_set=None):
    """
    Recursively generate JSON schema, excluding references and handling nested properties of 'object' and 'array' types.
    """
    if refs_set is None:
        refs_set = set()

    data_type = infer_data_type(data)

    if data_type == "object":
        properties = {}
        for k, v in data.items():
            if k == "$ref":
                continue  # Skip processing $ref entries.
            if isinstance(v, dict) and v.get('type') in ["object", "array"]:
                nested_properties = v.get('properties', {}) if v.get('type') == "object" else v.get('items', {}).get('properties', {})
                for nested_key, nested_value in nested_properties.items():
                    properties_result = _generate_json_schema_recursive({nested_key: nested_value}, refs_set)
                    if properties_result:
                        properties.update(properties_result['properties'])
            else:
                properties_result = _generate_json_schema_recursive(v, refs_set)
                if properties_result:
                    properties[k] = properties_result

        return {"type": "object", "properties": properties} if properties else None

    elif data_type == "array":
        items = data[0] if data else {}
        item_schema = _generate_json_schema_recursive(items, refs_set)
        return {"type": "array", "items": item_schema} if item_schema else None

    else:
        return {"type": data_type}

def generate_json_schema(sample_data, start_key=None):
    """
    Generate a JSON schema from a sample JSON data starting from a specified key.
    """
    try:
        if start_key:
            keys = start_key.split('.')
            for key in keys:
                sample_data = sample_data.get(key)
                if sample_data is None:
                    return None
        refs_set = set()
        schema = _generate_json_schema_recursive(sample_data, refs_set)
        if not schema:
            raise ValueError("No valid schema could be generated.")
        return schema
    except Exception as e:
        print(f"An error occurred: {e}")
        return None

def convert_json_to_struct(json_schema):
    """
    Convert JSON schema to PySpark StructType.
    """
    if json_schema and json_schema.get('type') == 'object':
        fields = [StructField(key, convert_json_to_struct(value), True) for key, value in json_schema.get('properties', {}).items()]
        return StructType(fields)
    elif json_schema and json_schema.get('type') == 'array':
        element_type = convert_json_to_struct(json_schema.get('items', {}))
        return ArrayType(element_type, True)
    else:
        return map_type(json_schema.get('type', 'null'))

def map_type(json_type):
    """
    Map JSON type to PySpark data type.
    """
    return {
        "string": StringType(),
        "integer": IntegerType(),
        "boolean": BooleanType(),
        "number": DoubleType(),
        "null": NullType()
    }.get(json_type, StringType())

def clean_column_name(name):
    """
    Clean column names by removing unnecessary suffixes and converting to snake case.
    """
    name = name.replace("properties", "").replace("type", "").replace("__", "_")
    name = re.sub(r'_\d+$', '', name)
    name = re.sub('([a-z0-9])([A-Z])', r'\1_\2', name).lower()
    name = re.sub(r'\s+', '_', name)
    return name.strip("_")

def generate_flatten_code(schema, parent_path="", used_names=None):
    """
    Recursively generate code to flatten DataFrame based on schema,
    ignoring 'example' fields, and excluding 'format', 'description' fields,
    and any fields ending with '_description'.
    """
    if used_names is None:
        used_names = set()
    flatten_parts = []

    for field in schema.fields:
        # Skip 'items', 'example', 'format', 'description' fields, and any field ending with '_description'
        if field.name == "items" or field.name.endswith("example") or field.name == "format" or field.name == "description" or field.name.endswith("_description"):
            continue

        field_path = f"{parent_path}{field.name}".strip(".")
        alias_name = clean_column_name(field.name)
        parent_clean_name = clean_column_name(parent_path.rstrip('.').split('.')[-1])
        original_alias = alias_name
        count = 1

        while alias_name in used_names:
            alias_name = f"{parent_clean_name}_{original_alias}" if parent_clean_name else original_alias
            if count > 1:
                alias_name += f"_{count}"
            count += 1

        alias_name = alias_name.strip("_")
        used_names.add(alias_name)

        if isinstance(field.dataType, StructType):
            nested_path = f"{field_path}."
            nested_code = generate_flatten_code(field.dataType, nested_path, used_names)
            flatten_parts.append(nested_code)
        elif isinstance(field.dataType, ArrayType) and isinstance(field.dataType.elementType, StructType):
            nested_path = f"{field_path}."
            array_code = generate_flatten_code(field.dataType.elementType, nested_path, used_names)
            flatten_parts.append(array_code)
        else:
            flatten_parts.append(f"col('{field_path}').alias('{alias_name}')")

    return ",\n    ".join(filter(None, flatten_parts))

def generate_pyspark_script(schema, output_file_path, schema_file_path):
    """
    Generate a PySpark script to load and process JSON data, including dynamic flattening.
    """
    if schema is None:
        raise ValueError("The schema is None, cannot generate PySpark script.")

    schema_json = json.dumps(schema.jsonValue(), indent=4)
    schema_json = schema_json.replace('true', 'True').replace('false', 'False')

    schema_file_stem = Path(schema_file_path).stem

    with open(schema_file_path, 'w') as schema_file:
        schema_file.write(f"""
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, ArrayType, MapType, BooleanType, LongType, DoubleType

def get_schema():
    return StructType.fromJson({schema_json})
""")

    flatten_code = generate_flatten_code(schema)
    flatten_code_lines = [line for line in flatten_code.split(",\n    ") if line.strip().split(".alias(")[1].strip(")").strip("'")]
    filtered_flatten_code = ",\n    ".join(flatten_code_lines)

    pyspark_code = f"""
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.sql.types import *
from pyspark.sql.functions import *
from {schema_file_stem} import get_schema

schema = get_schema()
spark = SparkSession.builder.appName("JsonToDataFrame").getOrCreate()
df = spark.read.schema(schema).option("multiLine", "true").json("input_data.json")

# Flatten the DataFrame to create individual columns for nested fields
df_flattened = df.select(
    {filtered_flatten_code}
)

# Show the flattened DataFrame
df_flattened.show(truncate=False)
"""

    with open(output_file_path, 'w') as file:
        file.write(pyspark_code)

def main():
    """
    Main function to execute the script. Reads configuration, processes JSON data, and generates a schema and a PySpark script.
    """
    try:
        base_directory = Path(__file__).parent.parent
        config_path = base_directory / 'config.yaml'
        print(config_path)

        with open(config_path, 'r') as file:
            config = yaml.safe_load(file)
        
        json_data_input_path = base_directory / (config['json_data_input_path'])
        json_schema_output_path = base_directory / (config.get('json_schema_output_path'))
        yaml_schema_output_path = base_directory / (config.get('yaml_schema_output_path'))
        pyspark_script_output_path = base_directory / (config.get('pyspark_script_output_path'))
        pyspark_schema_path = base_directory / (config.get('pyspark_schema_path'))
        start_key = config.get('start_key')

        if json_data_input_path.suffix == '.json':
            with open(json_data_input_path, 'r') as file:
                json_data = json.load(file)
        elif json_data_input_path.suffix == '.yaml' or json_data_input_path.suffix == '.yml':
            with open(json_data_input_path, 'r') as file:
                json_data = yaml.safe_load(file)
        else:
            raise ValueError("Unsupported file type. Only JSON and YAML are supported.")
        
        json_schema = generate_json_schema(json_data, start_key)

        if json_schema:
            with open(json_schema_output_path, 'w') as file:
                json.dump(json_schema, file, indent=4)

            with open(yaml_schema_output_path, 'w') as file:
                yaml.dump(json_schema, file, sort_keys=False)

            spark_schema = convert_json_to_struct(json_schema)

            generate_pyspark_script(spark_schema, pyspark_script_output_path, str(pyspark_schema_path))
        else:
            print("JSON schema generation failed.")
    except Exception as e:
        print(f"An error occurred: {e}")

if __name__ == "__main__":
    main()
